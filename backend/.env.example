# MarkDown OCR Backend Configuration
# Copy this file to .env and adjust the values as needed

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Choose your LLM provider: "ollama" or "lm_studio"
LLM_PROVIDER=lm_studio

# The specific model name to use (must be pulled/available in your local provider)
#
# Ollama examples:
#   - llama3: General purpose, good balance of speed and quality
#   - mistral: Faster, lighter model for quick conversions
#   - llava: Vision-capable model for image-heavy PDFs
#   - codellama: Optimized for documents with code blocks
#   - llama3.1: Latest version with improved reasoning
#
# LM Studio examples:
#   - Use the exact model name as shown in LM Studio
#   - Example: "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
LLM_MODEL=zai-org/glm-4.6v-flash

# =============================================================================
# Ollama Configuration
# =============================================================================

# URL for Ollama API (OpenAI-compatible endpoint)
# Default local: http://localhost:11434/v1
# Docker (macOS/Windows): http://host.docker.internal:11434/v1
# Docker (Linux): http://172.17.0.1:11434/v1
OLLAMA_BASE_URL=http://localhost:11434/v1

# =============================================================================
# LM Studio Configuration
# =============================================================================

# URL for LM Studio local server (OpenAI-compatible endpoint)
# Default: http://localhost:1234/v1
# Make sure to start the local server in LM Studio before using
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# =============================================================================
# Storage Configuration
# =============================================================================

# Directory for uploaded PDF files
UPLOAD_DIR=uploads

# Directory for converted Markdown files
OUTPUT_DIR=outputs

# =============================================================================
# Server Configuration
# =============================================================================

# API base path
API_V1_STR=/api/v1

# Project name (appears in API docs)
PROJECT_NAME=MarkDown OCR

# =============================================================================
# CORS Configuration
# =============================================================================

# Allowed origins for CORS (comma-separated)
# Use "*" for development, restrict for production
# Example: http://localhost:3000,https://yourdomain.com
CORS_ORIGINS=*

# =============================================================================
# Advanced LLM Settings (Optional)
# =============================================================================

# Timeout for LLM requests in seconds
# LLM_TIMEOUT=60

# Maximum number of retries for failed LLM calls
# LLM_MAX_RETRIES=2

# Temperature for LLM generation (0.0 = deterministic, 1.0 = creative)
# LLM_TEMPERATURE=0.0

# =============================================================================
# Performance Tuning (Optional)
# =============================================================================

# Maximum file size for uploads in MB
# MAX_UPLOAD_SIZE=50

# Number of concurrent pages to process
# CONCURRENT_PAGES=1

# Enable debug logging
# DEBUG=false
